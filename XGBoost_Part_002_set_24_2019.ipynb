{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost Part 002 set 24 2019.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/LabFiq/blob/master/XGBoost_Part_002_set_24_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MpmNaCYrrp7",
        "colab_type": "text"
      },
      "source": [
        "# **XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NOrC60qblZ",
        "colab_type": "text"
      },
      "source": [
        "XGBoost is a scalable machine learning system for tree boosting. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7T-RIf-zXhL",
        "colab_type": "text"
      },
      "source": [
        "XGBoost as other boost models combines the predictions of several\n",
        "models to results in a model with improved predictive\n",
        "performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwgqUe1mwTYF",
        "colab_type": "text"
      },
      "source": [
        "XGBoost (as result of work as  non-parametric model with no formal distributional assumptions) can handle skewed and multi-modal data as well as categorical data that are ordinal or non-ordinal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_oe8HVCuOqv",
        "colab_type": "text"
      },
      "source": [
        "ADVANTAGES \n",
        "\n",
        "1.   Can be used with a variety of response types (binomial, gaussian, poisson)\n",
        "2.   Stochastic, which improves predictive performance\n",
        "The best fit is automatically detected by the algorithm\n",
        "3. Model represents the effect of each predictor after accounting for the effects of other predictors\n",
        "4. Robust to missing values and *outliers*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYg_04WmvDBc",
        "colab_type": "text"
      },
      "source": [
        "XGBoost is available as an open source package and thereâ€™re various high-level interfaces. Currently there are interfaces of XGBoost in C++, R, Python, Julia, Java and Scala"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiJjM2tmvK8d",
        "colab_type": "text"
      },
      "source": [
        "### REFERENCES \n",
        "1. https://support.bccvl.org.au/support/solutions/articles/6000083212-generalized-boosting-model\n",
        "2. https://arxiv.org/pdf/1603.02754.pdf  XGBoost: A Scalable Tree Boosting System-Tianqi Chen and Carlos Guestrin - University of Washington\n",
        "3. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.276&rep=rep1&type=pdf  The State of Boosting-Greg Ridgeway\n",
        "Department of Statistics University of Washington"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHDYQiA8mk6C",
        "colab_type": "text"
      },
      "source": [
        "**IMPORTANCE OF VARIABLES**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukk8XdQHlzUC",
        "colab_type": "text"
      },
      "source": [
        "https://datascience.stackexchange.com/questions/12318/how-to-interpret-the-output-of-xgboost-importance?source=post_page-----6e16132588e7----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdIG_EIjmA64",
        "colab_type": "text"
      },
      "source": [
        "The xgboost to fit boosted trees for binary classification. \n",
        "\n",
        "The importance matrix is actually a data.table object with the first column listing the names of all the features actually used in the boosted trees.\n",
        "\n",
        "The meaning of the importance data table is as follows:\n",
        "\n",
        "1. The Gain implies the relative contribution of the corresponding feature to the model calculated by taking each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.\n",
        "2.The Cover metric means the relative number of observations related to this feature. For example, if you have 100 observations, 4 features and 3 trees, and suppose feature1 is used to decide the leaf node for 10, 5, and 2 observations in tree1, tree2 and tree3 respectively; then the metric will count cover for this feature as 10+5+2 = 17 observations. This will be calculated for all the 4 features and the cover will be 17 expressed as a percentage for all features' cover metrics.\n",
        "3. The Frequency  is the percentage representing the relative number of times a particular feature occurs in the trees of the model. In the above example, if feature1 occurred in 2 splits, 1 split and 3 splits in each of tree1, tree2 and tree3; then the weightage for feature1 will be 2+1+3 = 6. The frequency for feature1 is calculated as its percentage weight over weights of all features.\n",
        "\n",
        "The Gain is the most relevant attribute to interpret the relative importance of each feature."
      ]
    }
  ]
}